

import cv2
import numpy as np
import glob as gb
import matplotlib.pyplot as plt
import pandas as pd 
import psycopg2 as ps
%matplotlib inline
import os
import seaborn as sns
from sklearn import metrics 
from sklearn.model_selection import train_test_split

from sklearn.neighbors import KNeighborsClassifier
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB

from google.colab import files

img_width = 600
img_height = 450

!pip install -q wordcloud
import wordcloud

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger') 

import io
import unicodedata

import re
import string

!sudo apt install tesseract-ocr
!pip install pytesseract

import pytesseract
import shutil
import os
import random
from pytesseract import Output
try:
 from PIL import Image
except ImportError:
 import Image

def pre_process(name):
    img = cv2.imread(name, 1)
    img_resized = cv2.resize(img, (img_width, img_height))
    img_blur = cv2.GaussianBlur(img_resized, (3, 3), 1)
    #img_show=cv2.imshow('image', img)
    return img_resized, img_blur

  


def pre_process_rot(name):
    img_resized = cv2.resize(name, (img_width, img_height))
    img_blur = cv2.GaussianBlur(img_resized, (3, 3), 0)
    #img_show=cv2.imshow('image', img)
    return img_resized, img_blur



#retrieve uploaded file
uploaded = files.upload()
#print results
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

def draw(img, lines):
    new_img = img.copy()
    for rho, theta in lines[:]:
        a = np.cos(theta)
        b = np.sin(theta)
        x0 = a * rho
        y0 = b * rho
        x1 = int(x0 + 1000 * (-b))
        y1 = int(y0 + 1000 * a)
        x2 = int(x0 - 1000 * (-b))
        y2 = int(y0 - 1000 * a)
        cv2.line(new_img, (x1, y1), (x2, y2), (0, 0, 255), 2)
    return new_img
  

def draw_vertical(img, lines):
    new_img = img.copy()
    for rho, theta in lines[:]:
        x1 = rho
        x2 = rho
        y1 = 0
        y2 = img_width
        cv2.line(new_img, (x1, y1), (x2, y2), (0, 0, 255), 2)
    return new_img


def line_reduce(lines):
    i = 0
    j = 0
    lines_final = []
    while i < len(lines) - 1:
        if j >= len(lines) - 1:
            break
        j = i + 1
        lines_final.append(lines[i])
        while j < len(lines) - 1:
            if lines[j][0] - lines[i][0] > 10:
                i = j
                break
            else:
                j = j + 1
    return lines_final

def line_shifting(lines_list):
    lines = []
    for rho, theta in lines_list[:]:
        if (theta < (np.pi / 6.0)) or (theta > (11 * np.pi / 6.0)) or ((theta > (5 *np.pi / 6.0)) and (theta < (7 * np.pi / 6.0))):  # Limit lines with angle less than 30 degrees to the y-axis
            lines.append([rho, theta])
    lines.sort()
    lines_final = line_reduce(lines)
    return lines_final



# ------------Region Grow---------------
class Point(object):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def get_x(self):
        return self.x

    def get_y(self):
        return self.y

def get_seeds(lines):
    seeds = []
    i = 0
    j = 1
    while i < len(lines)-2:
        y = int(lines[i][0] + (lines[j][0] - lines[i][0])/2) # The x and y of the image index are opposite to the x and y we understand
        x = int(img_height/2)
        seeds.append(Point(x, y))
        i = i + 1
        j = j + 1
    return seeds

def get_gray_diff(img, current_point, adjacent_point):
    return abs(int(img[current_point.x][current_point.y]) - int(img[adjacent_point.x][adjacent_point.y]))

def get_connects():
    connects = [Point(-1, -1), Point(-1, 0), Point(-1, 1), Point(0, -1), Point(0, 1), Point(1, -1), Point(1, 0),
                Point(1, 1)]
    return connects

def region_grow(img, seeds, thresh):
    seed_mark = np.zeros(img.shape)
    seed_stack = []
    for seed in seeds:
        seed_stack.append(seed)
    mark = 1
    connects = get_connects()
    while len(seed_stack) > 0:
        current_point = seed_stack.pop(0)
        seed_mark[current_point.x][current_point.y] = mark
        for connect in connects:
            adjacent_x = int(current_point.x + connect.x)
            adjacent_y = int(current_point.y + connect.y)
            if adjacent_x < 0 or adjacent_y < 0 or adjacent_x >= img_height or adjacent_y >= img_width:
                continue
            gray_diff = get_gray_diff(img, current_point, Point(adjacent_x, adjacent_y))
            if gray_diff < thresh and seed_mark[adjacent_x][adjacent_y] == 0:
                seed_mark[adjacent_x][adjacent_y] = mark
                seed_stack.append(Point(adjacent_x, adjacent_y))
    return seed_mark

# --------------image segmentation---------------
def segmentation(img, lines):
    imgs = []
    i = 0
    j = 1
    while i < len(lines) -2:
        x1 = int(lines[i][0])
        x2 = int(lines[j][0])
        book_img = img[0:img_height, x1:x2]
        imgs.append(book_img)
        i = i + 1
        j = j + 1

    return imgs

def seg_horizontal(img):
    thresh = img.shape[1] - 10
    edges = cv2.Canny(img, 50, 150, apertureSize=3)
    lines_pre = cv2.HoughLines(edges, 1, np.pi / 180, thresh)  #The last parameter can be adjusted, which will affect the effect of straight line detection 
    lines = lines_pre[:, 0, :]
    lines_horizontal = []
    for rho, theta in lines[:]:
        if ((theta < (12 * np.pi / 18.0)) and (theta > (4 * np.pi / 18.0))) or ((theta > (22 * np.pi / 18.0)) and (theta < (32 * np.pi / 18.0))):
            lines_horizontal.append([rho, theta])
    lines_horizontal.sort()
    lines_horizontal = line_reduce(lines_horizontal)
    if len(lines_horizontal) == 0:
        return img
    y1 = int(lines_horizontal[0][0])
    y2 = int(lines_horizontal[len(lines_horizontal)-1][0])
    book_img = img[y1:y2, 0:img_width]
    return book_img


from google.colab.patches import cv2_imshow

array=[]
def main(img_path):
    
    global img_show
    print (img_path)
    for path in img_path:
        img_gray, img = pre_process(path)
        cv2_imshow(img_gray)
        edges = cv2.Canny(img, 50, 150, apertureSize=3)
        kernel = np.ones((1,1),np.uint8)
        erosion = cv2.erode(edges,kernel,iterations = 1)
        cv2_imshow(erosion)
        lines = cv2.HoughLines(erosion, 1, np.pi/180, 160)  # The last parameter can be adjusted, which will affect the effect of straight line detection
        lines1 = lines[:, 0, :]
        print(lines1)
        houghlines = line_shifting(lines1)  #Store and filter detected vertical lines
        #print(houghlines)
        #houghlines = lines1
        img_show = draw(img_gray, houghlines)
        
        img_segmentation = segmentation(img_gray, houghlines)
        i = 0
        for img_s in img_segmentation:
            if img_s.shape[0] == 0:
                print(i)
            #path1='result/'
            #cv2.imwrite(path1+'results_new' + str(i) + '.jpg',img_s)
            array.append(img_s)
            
            i = i+1
        
        
        cv2_imshow(img_show)
        
        #cv2.waitKey() 
        #return array 
main(gb.glob("book1-ps2.jpg"))

def trans(img_path):
    
    #global img_show
    
    #for path in img_path:
        img_gray, img = pre_process_rot(img_path)
        edges = cv2.Canny(img, 50, 150, apertureSize=3)
        lines = cv2.HoughLines(edges, 1, np.pi/180, 160)  # The last parameter can be adjusted, which will affect the effect of straight line detection
        lines1 = lines[:, 0, :]
        houghlines = line_sifting(lines1)  #Store and filter detected vertical lines
        img_show = draw_vertical(img_gray, houghlines)
        img_segmentation = segmentation(img_gray, houghlines)
        i = 0
        for img_s in img_segmentation:
            if img_s.shape[0] == 0:
                print(i)
            #path1='result/'
            #cv2.imwrite(path1+'results_new' + str(i) + '.jpg',img_s)
            array.append(img_s)
            
            i = i+1
        
        
        #cv2_imshow(img_show)
        #cv2.waitKey() 
        return array 



if len(array)==0:
   img_show=cv2.transpose(img_show)
   img_show=cv2.flip(img_show,flipCode=1)
   trans(img_show)

cv2_imshow(img_show)


i=1
for x in array:
    print(' ')
    out=cv2.transpose(x)
    out=cv2.flip(out,flipCode=0)
    cv2_imshow(out)
    i=i+1

ocr_arr=[]

for a in array:
  a=cv2.transpose(a)
  a=cv2.flip(a,flipCode=0)
  print(' ')
  cv2_imshow(a)
  
  #converting image into gray scale image
  gray = cv2.cvtColor(a, cv2.COLOR_BGR2GRAY)
  print(' ')
  cv2_imshow(gray)
  
  # converting it to binary image by Thresholding

  # this step is require if you have colored image because if you skip this part

  # then tesseract won't able to detect text correctly and this will give incorrect result
  (thresh, im_bw) = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
  print(' ')
  cv2_imshow(im_bw)
  
  #configuring parameters for tesseract
  custom_config = r'--oem 3 --psm 6'
  # now feeding image to tesseract
  details= pytesseract.image_to_data(gray, output_type=Output.DICT, config=custom_config, lang='eng')
  print(details)

  n_boxes = len(details['level'])
  for i in range(n_boxes):
      (x, y, w, h) = (details['left'][i], details['top'][i], details['width'][i], details['height'][i])
      cv2.rectangle(im_bw, (x, y), (x + w, y + h), (0, 255, 0), 2)

  print(details)
  cv2_imshow(im_bw)

  parse_text = []
  word_list = []
  last_word = ''
  for word in details['text']:
    if word!='':
      word_list.append(word)
      last_word = word
    if (last_word!='' and word == '') or (word==details['text'][-1]):
      parse_text.append(word_list)
      #word_list = []

  print(word_list)
  p=(" ".join(map(str,word_list)))
  print(p)
  ocr_arr.append(p)

i=1
for x in ocr_arr:
    print(i)
    print(x)
    i=i+1   

ocr_arr=[]

for a in array:
  a=cv2.transpose(a)
  a=cv2.flip(a,flipCode=0)
  #cv2_imshow(a)
  
  gray = cv2.cvtColor(a, cv2.COLOR_BGR2GRAY)
  #cv2_imshow(gray)
  
  (thresh, im_bw) = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
  #cv2_imshow(im_bw)

  rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (18, 18)) 

  # Appplying dilation on the threshold image 
  dilation = cv2.dilate(im_bw, rect_kernel, iterations = 1) 
    
  # Finding contours 
  contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)

  # Looping through the identified contours 
  # Then rectangular part is cropped and passed on 
  # to pytesseract for extracting text from it 
  # Extracted text is then written into the text file 
  for cnt in contours: 
      x, y, w, h = cv2.boundingRect(cnt) 
        
      # Drawing a rectangle on copied image 
      rect = cv2.rectangle(im_bw, (x, y), (x + w, y + h), (0, 255, 0), 2) 
        
      # Cropping the text block for giving input to OCR 
      cropped = im_bw[y:y + h, x:x + w] 
        
      # Apply OCR on the cropped image 
      text = pytesseract.image_to_string(cropped) 
      ocr_arr.append(text)

  #print(text)
i=1
for x in ocr_arr:
  print(i)
  print(x)
  i=i+1   

a=cv2.transpose(array[20])
a=cv2.flip(a,flipCode=0)
cv2_imshow(a)


gray = cv2.cvtColor(a, cv2.COLOR_BGR2GRAY)
cv2_imshow(gray)

(thresh, im_bw) = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
imgs=im_bw
cv2_imshow(im_bw)
cv2_imshow(imgs)


from pytesseract import Output
#configuring parameters for tesseract
custom_config = r'--oem 3 --psm 6'
# now feeding image to tesseract
details= pytesseract.image_to_data(gray, output_type=Output.DICT, config=custom_config, lang='eng')
print(details)

n_boxes = len(details['level'])
for i in range(n_boxes):
    (x, y, w, h) = (details['left'][i], details['top'][i], details['width'][i], details['height'][i])
    cv2.rectangle(im_bw, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2_imshow(im_bw)

parse_text = []
word_list = []
last_word = ''
for word in details['text']:
  if word!='':
    word_list.append(word)
    last_word = word
  if (last_word!='' and word == '') or (word==details['text'][-1]):
    parse_text.append(word_list)
    #word_list = []

print(word_list)
print (" ".join(map(str,word_list)))

rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (18, 18)) 

# Appplying dilation on the threshold image 
dilation = cv2.dilate(imgs, rect_kernel, iterations = 1) 
  
# Finding contours 
contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)

# Looping through the identified contours 
# Then rectangular part is cropped and passed on 
# to pytesseract for extracting text from it 
# Extracted text is then written into the text file 
for cnt in contours: 
    x, y, w, h = cv2.boundingRect(cnt) 
      
    # Drawing a rectangle on copied image 
    rect = cv2.rectangle(imgs, (x, y), (x + w, y + h), (0, 255, 0), 2) 
      
    # Cropping the text block for giving input to OCR 
    cropped = imgs[y:y + h, x:x + w] 
      
    # Apply OCR on the cropped image 
    text = pytesseract.image_to_string(cropped) 

print(text)

!sudo apt install tesseract-ocr

!pip install pytesseract

import pytesseract
import shutil
import os
import random
try:
 from PIL import Image
except ImportError:
 import Image

ocr_arr=[]

for x in array:
    out=cv2.transpose(x)
    out=cv2.flip(out,flipCode=0)
    extractedInformation = pytesseract.image_to_string(Image.fromarray(out))
    text=str(extractedInformation)
    ocr_arr.append(text)
    #print(text)

i=1
for x in ocr_arr:
    print(i)
    print(x)
    i=i+1   

# xlrd is needed to load excel files
!pip install xlrd
!pip install psycopg2
!pip install pymysql

#retrieve uploaded file
uploaded = files.upload()
#print results
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

column_names = ["File_name", "Book_Title" , "Author", "Edition", "Product_Type", "Copyright_Year" , "Language", "Language_Collection", "Series_Title", "Subject_Classification" ]

excel_path = "datasets_627441_1117797_Metadata - Copy1.xlsx"

booklist_data = pd.read_excel(excel_path, "Booklist", header=None, names=column_names) 
#summary_data  = pd.read_excel(excel_path, "Summary", header=None, names=column_names) 


# we need to reset the index to have nice linear indices from 0, 1 to 150
# reset_index() adds a new column called "index" which we drop (explained later)
book = pd.concat([booklist_data])

#book=book.drop([0,403,404], axis=0)
book.head(600)

feature = ['Book_Title','Author','Subject_Classification']
book=book[feature]
book

!pip3 install fuzzywuzzy[speedup]


# fuzz is used to compare TWO strings
from fuzzywuzzy import fuzz

# process is used to compare a string to MULTIPLE other strings
from fuzzywuzzy import process

list=[]
global score
for x in ocr_arr:
  max=0
  for ind2 in book.index:
    score=fuzz.token_set_ratio(x, book['Book_Title'][ind2])
    #list.append(score)
    #for i in list:
    if score>max:
      max=score
      z=ind2
    
  if max>0:
    print(book['Book_Title'][z],"  -->  ",book['Subject_Classification'][z])
    #print(max)
    list.append(book['Subject_Classification'][z])

print(list) 

temp=set(list)
result={}
for i in temp:
    result[i]=list.count(i)
    print (i, "->", result[i])











#retrieve uploaded file
uploaded = files.upload()
#print results
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

sample=pd.read_csv('Edit_bangla_book - Copy.csv',encoding='utf8',error_bad_lines=False)
#sample.apply(lambda x: pd.lib.infer_dtype(x.values))
sample.head(200)

sample['Title']

feature = ['Title','Author','Category']
bookbg=sample[feature]
bookbg

from gensim.models import Word2Vec

pip install bnlp_toolkit

from bnlp.basic_tokenizer import BasicTokenizer
basic_t = BasicTokenizer()
#raw_text = "আমি বাংলায় গান গাই।"
vec=[]
for ind in bookbg.index:
  raw_text =bookbg['Title'][ind]
  tokens = basic_t.tokenize(raw_text)
  print(tokens)
  vec.append(tokens)

print(vec)

model = Word2Vec(bookbg.Title, size=500, window=5, min_count=1)

model
print(model)
print(model.wv.vocab)

ar=['সুশ সনের সন্ধানে', 'শেষের কবিতা', 'নৌক ডুবি', 'গোরা']

for i in model.wv.vocab:
  print(i)

def vocab_list(name):
  model1 = Word2Vec(name, size=500, window=5, min_count=1)
  list=[]
  for i in model1.wv.vocab:
    list.append(i)
   #print(list)
  return list 

#vocab_list(ar)
#for i in ar:
  #vocab_list(i)
a=vocab_list('সুশাসনের সন্ধানে')  
print(a)

list0=[]
global score
for x in ar:
  max=1
  list1=vocab_list(x)
  for ind2 in book.index:
    #score=fuzz.token_set_ratio(x, book['Book_Title'][ind2])
    list2=vocab_list(bookbg['Title'][ind2])
    res = len(set(list1) & set(list2)) / float(len(set(list1) | set(list2))) * 100
    
    if res>max:
      max=res
      z=ind2
    
  if max>1:
    print(bookbg['Category'][z],"  ", bookbg['Title'][z])
    print(max)
    list0.append(bookbg['Category'][z])

print(list0) 

temp1=set(list0)
result1={}
for i in temp1:
    result1[i]=list0.count(i)
    print (i, "->", result1[i])













def replace_strings(texts, replace):
    new_texts=[]
    
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)
    
    for text in texts:
        for r in replace:
            text=text.replace(r[0], r[1])
        text=emoji_pattern.sub(r'', text)
        text=english_pattern.sub(r'', text)
        text=re.sub(r'\s+', ' ', text).strip()
        new_texts.append(text)

    return new_texts

def remove_punc(sentences):
    # import ipdb; ipdb.set_trace()
    new_sentences=[]
    exclude = list(set(string.punctuation))
    exclude.extend(["’", "‘", "—"])
    for sentence in sentences:
        s = ''.join(ch for ch in sentence if ch not in exclude)
        new_sentences.append(s)
    
    return new_sentences


ebala_body=bookbg['Title']

print("\x1b[31mCrawled Unprocessed Text\x1b[0m")
print(ebala_body[43])

replace=[('\u200c', ' '),
         ('\u200d', ' '),
        ('\xa0', ' '),
        ('\n', ' '),
        ('\r', ' ')]

ebala_body=remove_punc(ebala_body)

print("\x1b[31mSentences after removing all punctuations\x1b[0m")
print(ebala_body[43])

ebala_body=replace_strings(ebala_body, replace)

print("\x1b[31mSentences after replacing strings\x1b[0m")
print(ebala_body[43])


# Import pymysql module
import pymysql
import sqlite3

y=book.Subject_Classification
x=book.drop('Subject_Classification',axis=1)
x=x.drop('Series_Title',axis=1)
x=x.drop('Language',axis=1)
x=x.drop('File_name',axis=1)
x=x.drop('Product_Type',axis=1)

x=x.drop('Language_Collection',axis=1)
x=x.drop('Copyright_Year',axis=1)
x=x.drop('Edition',axis=1)



book.Subject_Classification.unique()

conn = sqlite3.connect('TestDB1.db')
c = conn.cursor()

c.execute('CREATE TABLE Books_Detail (File_name, Book_Title, Author, Edition, Product_Type, Copyright_Year, Language, Language_Collection, Series_Title, Subject_Classification)')
conn.commit()

book.to_sql('BOOKS', conn, if_exists='replace', index = False)


c.execute("BOOKS")

for row in c.fetchall():
    print (row)

# Convert to lower case
x["Book_Title"] = x["Book_Title"].str.lower()
x["Author"] = x["Author"].str.lower()
#x["Edition"] = x["Edition"].str.lower()
y = y.str.lower()

x

import string
PUNCT_TO_REMOVE = string.punctuation
def remove_punctuation(text):
    """custom function to remove the punctuation"""
    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))

#df_data["text_no_punct"] = df_data["text"].apply(lambda text: remove_punctuation(text))
x["Book_Title"] = x["Book_Title"].apply(lambda text: remove_punctuation(text))
x["Author"] = x["Author"].apply(lambda text: remove_punctuation(text))
#x["Edition"] = x["Edition"].apply(lambda text: remove_punctuation(text))
y = y.apply(lambda text: remove_punctuation(text))
x

def remove_whitespace(text): 
     
    return " ".join(text.split())

    
#df_data["text"] = df_data["text"].apply(lambda text: remove_whitespace(text))
x["Book_Title"] = x["Book_Title"].apply(lambda text: remove_whitespace(text))
x["Author"] = x["Author"].apply(lambda text: remove_whitespace(text))
#x["Edition"] = x["Edition"].apply(lambda text: remove_whitespace(text))
y = y.apply(lambda text: remove_whitespace(text))

x

#df.columns = df.columns.str.replace(' ', '_')
x["Book_Title"] = x["Book_Title"].str.replace(' ', '')
x["Author"] = x["Author"].str.replace(' ', '')
#x["Edition"] = x["Edition"].str.replace(' ', '')
y = y.str.replace(' ', '')

x

from sklearn import preprocessing

number = preprocessing.LabelEncoder()
x['Book_Title'] = number.fit_transform(x.Book_Title)
x['Author'] = number.fit_transform(x.Author)

y = number.fit_transform(y)
y

#x=x.drop([403,404], axis=0)
#y=y.drop([403,404], axis=0)
regressor = LogisticRegression()  
regressor.fit(x, y) #training the algorithm

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.02) 
x_test

ar=["Statistics","Essentials of Food Science","A First Introduction to Quantum Physics","Differential Equations and Their Applications"]

for x in ar:
  for ind2 in book.index:
    if x in book["Book_Title"][ind2]: print(book["Subject_Classification"][ind2]) 
    #print(book[book["Book_Title"][ind2].str.contains(x).any()].Subject_Classification)

!pip3 install fuzzywuzzy[speedup]

# fuzz is used to compare TWO strings
from fuzzywuzzy import fuzz

# process is used to compare a string to MULTIPLE other strings
from fuzzywuzzy import process

list=[]
global score
for x in ocr_arr:
  max=1
  for ind2 in book.index:
    score=fuzz.token_set_ratio(x, book['Book_Title'][ind2])
    #list.append(score)
    #for i in list:
    if score>max:
      max=score
      z=ind2
    
  if max>1:
    print(book['Subject_Classification'][z],"  ", book['Book_Title'][z])
    print(max)
    list.append(book['Subject_Classification'][z])

print(list)  

temp=set(list)
result={}
for i in temp:
    result[i]=list.count(i)
    print (i, "->", result[i])

 

y_pred=regressor.predict(x_test)

y_pred

y_test

confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])
sns.heatmap(confusion_matrix, annot=True)

print('Accuracy: ',metrics.accuracy_score(y_test, y_pred)*100)
plt.show()

dfinal=pd.DataFrame(
{
    'Book_Title': x_test['Book_Title'],
    'Category':y_pred
})

dfinal



# -*- coding: utf-8 -*-
"""
Created on Sat Apr 21 13:10:49 2018
@author: Shaojun Luo
"""
import numpy as np
import cv2

# define value of HORIZONTAL
HORIZONTAL = 1
VERTICAL = -1
LEFT = -1
RIGHT = 1
UP = -1
DOWN = 1
max_gap = 4
# default parameters
EDParam_default = {'ksize': 5,
                   'sigma': 1.0,
                   'gradientThreshold': 36, 
                   'anchorThreshold': 8,
                   'scanIntervals': 1}

class EdgeDrawing:
    # initiation 
    def __init__(self, EDParam = EDParam_default):
        #	EDLineDetector constructor function
        #set parameters for line segment detection
        self.ksize_ = EDParam['ksize']
        self.sigma_ = EDParam['sigma']
        self.gradientThreshold_ = EDParam['gradientThreshold']
        self.anchorThreshold_  = EDParam['anchorThreshold']
        self.scanIntervals_    = EDParam['scanIntervals']
        #dimension of image
        self.MAX_X = 0 
        self.MAX_Y = 0
        self.G_ = np.array([])
        self.D_ = np.array([])
        self.E_ = np.array([])
        
    # search algorithm for EdgeDrawing
    def GoUp_(self,x, y):
        segment = [] # array to record edge segment
        direct_next = None # search direction of left side similart to right, up and down  
        while x>0 and self.G_[x,y]>0 and not self.E_[x,y]:
            next_y = [max(0,y-1), y, min(self.MAX_Y-1,y+1)] # search in a valid area
            segment.append((x,y))# extend line segments
            if self.D_[x,y] == VERTICAL:
                self.E_[x, y] = True # mark as edge
                y_last = y # record parent pixel
                x, y = x-1, next_y[np.argmax(self.G_[x-1, next_y])]# walk to next pixel with max gradient
            else:
                direct_next = y - y_last # change direction to continue search
                break # stop and proceed to next search
        return segment,direct_next
        
    def GoDown_(self,x, y):
        segment = []
        direct_next = None
        while x < self.MAX_X-1 and self.G_[x,y]>0 and not self.E_[x, y]:
            next_y = [max(0,y-1), y, min(self.MAX_Y-1,y+1)]
            segment.append((x,y))
            if self.D_[x,y] == VERTICAL:
                self.E_[x, y] = True
                y_last = y
                x, y = x+1, next_y[np.argmax(self.G_[x+1, next_y])]
            else:
                direct_next = y - y_last
                break
        return segment,direct_next
            
    def GoRight_(self,x, y):
        segment = []
        direct_next = None
        while y < self.MAX_Y-1 and self.G_[x,y]>0 and not self.E_[x,y]:
            next_x =  [max(0,x-1), x, min(self.MAX_X-1,x+1)]
            segment.append((x,y))
            if self.D_[x,y] == HORIZONTAL:
                self.E_[x, y] = True
                x_last = x
                x, y = next_x[np.argmax(self.G_[next_x, y+1])], y+1
            else:
                direct_next = x - x_last
                break
        return segment,direct_next
    
    def GoLeft_(self,x, y):
        segment = []
        direct_next = None
        while y>0 and self.G_[x, y]>0 and not self.E_[x, y]:
            next_x = [max(0,x-1), x, min(self.MAX_X-1,x+1)]
            segment.append((x,y))
            if self.D_[x,y] == HORIZONTAL:
                self.E_[x, y] = True
                x_last = x
                x, y = next_x[np.argmax(self.G_[next_x, y-1])], y-1
            else:
                direct_next = x - x_last
                break
        return segment, direct_next
    
    # walk down until reach the end
    def SmartWalk_(self, x,y,direct_next):
        segment = [(x,y)]
        while direct_next is not None:
            x, y = segment[-1][0], segment[-1][1]
            # if the last point of chain is horizontal, explore horizontally
            if self.D_[x,y] == HORIZONTAL:
                # get segment sequence
                if direct_next == LEFT:
                    s, direct_next = self.GoLeft_(x,y)
                elif direct_next == RIGHT:
                    s, direct_next = self.GoRight_(x,y)
                else: break
#                    if self.G_[x,y+1]>self.G_[x,y-1]:
#                        s, direct_next = self.GoRight_(x,y)
#                    else:
#                        s, direct_next = self.GoLeft_(x,y)
            elif self.D_[x,y] == VERTICAL:  # explore vertically
                if direct_next == UP:
                    s, direct_next = self.GoUp_(x,y)
                elif direct_next == DOWN:
                    s, direct_next = self.GoDown_(x,y)
                else: break
#                    if self.G_[x-1,y]>self.G_[x+1,y]:
#                        s, direct_next = self.GoUp_(x,y)
#                    else:
#                        s, direct_next = self.GoDown_(x,y)
            else: # if the next pixel is invalid
                break
            if len(s) > 1:
                segment.extend(s[1:])
        return segment
    # find list of anchors
    def FindAnchors_(self,image):
        # detect the anchor
        anchor_list = []
        for i in range(1,image.shape[0]-1,self.scanIntervals_):
            for j in range(1,image.shape[1]-1,self.scanIntervals_):
                if self.D_[i,j] == HORIZONTAL:  # HORIZONTAL EDGl compare up & down
                    if self.G_[i,j] - self.G_[i-1,j] >= self.anchorThreshold_ and self.G_[i,j] - self.G_[i+1,j] >= self.anchorThreshold_:
                        anchor_list.append((i,j))
                elif self.D_[i,j] == VERTICAL: # VERTICAL EDGE. Compare with left & right.
                    if self.G_[i,j] - self.G_[i,j-1] >= self.anchorThreshold_ and self.G_[i,j] - self.G_[i,j+1] >= self.anchorThreshold_:                    
                        anchor_list.append((i,j))
        return anchor_list
    # merge edges
    def MergeEdges_(self,edges):
        # connect and merge the edges inplace
        merged = True
        while merged: # if last iteration perform merged
            p1 = 0 # pivot for first edge
            merged = False # assume not going to merge
            # iterate over edges to merge
            while p1 < len(edges):
                p2 = p1+1
                while p2 < len(edges):
                    # mark start and end point of 2 segments
                    start_1, end_1 = edges[p1][0], edges[p1][-1]
                    start_2, end_2 = edges[p2][0], edges[p2][-1]
                    # direction of two vectors
                    v_1 = (end_1[0]-start_1[0],end_1[1]-start_1[1])
                    v_2 = (end_2[0]-start_2[0],end_2[1]-start_2[1])
                    # if they aligned in the same direction, compare with head-end
                    if np.dot(v_1,v_2)>=0:
                        if abs(end_1[0] - start_2[0]) + abs(end_1[1] - start_2[1]) < max_gap:
                            # merge end-head
                            edges[p1] =  edges[p1] + edges.pop(p2)
                            merged = True
                        elif abs(start_1[0] - end_2[0])+abs(start_1[1] - end_2[1]) < max_gap:
                            # merge end-head
                            edges[p1] =  edges.pop(p2)+ edges[p1]
                            merged = True
                        else:
                            p2 += 1 # manually poceed to next segment
                    else:
                        if abs(start_1[0] - start_2[0]) + abs(start_1[1]-start_2[1]) < max_gap:
                            # merge head-head
                            edges[p1] = edges[p1][::-1] + edges.pop(p2)
                            merged = True
                        elif abs(end_1[0] - end_2[0]) + abs(end_1[1] - end_2[1]) < max_gap:
                            # merge end-end
                            edges[p1] = edges.pop(p2) + edges[p1][::-1]
                            merged = True
                        else:
                            p2 += 1 # manually poceed to next segment
                p1 += 1 # next segment
        return
    # edge drawing algorithm
    def EdgeDrawing(self, image, smoothed = False):
        # validation check for image
        if len(image.shape)>2:
            raise('Use only 1 channel or grayscale image')
            return None
        # set up dimension
        self.MAX_X = image.shape[0]
        self.MAX_Y = image.shape[1]
        # if not smoothed then smooth it
        if not smoothed: #input image hasn't been smoothed.
            img = cv2.GaussianBlur(image,(self.ksize_,self.ksize_),self.sigma_)
        else:
            img = image.copy()
        # compute dx,dy imagegradient
        dxImg_ = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=1)
        dyImg_ = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=1)
        
        # Compute gradient map and direction map
        #self.G_ = np.sqrt(dxImg_*dxImg_ + dyImg_*dyImg_)
        self.G_ = np.abs(dxImg_)+ np.abs(dyImg_)
        self.G_[self.G_ < self.gradientThreshold_] = 0  
        # If true, then it is horizontal edge
        self.D_ = -np.sign(np.abs(dxImg_) - np.abs(dyImg_))
        self.D_[self.G_ < self.gradientThreshold_] = 0
        
        #cv2.imwrite('Gradient.bmp',255*(self.G_>0).astype(int))
        # find anchor list
        anchor_list = self.FindAnchors_(image)                        
                
        edges = []
        # initiate edgemap
        self.E_ = np.zeros(self.G_.shape,dtype = bool)
        # first round edrawing, get fragment segments
        for anchor in anchor_list:
            if not self.E_[anchor]: # if not mark as edges
                # walk right or down
                segment_1 = self.SmartWalk_(anchor[0], anchor[1], 1)
                # reset anchor point
                self.E_[anchor] = False
                # walk left or up
                segment_2 = self.SmartWalk_(anchor[0], anchor[1], -1)
                # concat two segments
                if len(segment_1[::-1] + segment_2)>0:
                    edges.append(segment_1[::-1] + segment_2[1:])
        # merge the edges with same direction
        #self.MergeEdges_(edges)
        edge_map = 255*self.E_.astype(np.uint8)
        return edges, edge_map

        EdgeDrawing(self, '1.jpg', smoothed = False)

        cv2_imshow(img)

import numpy as np
from numpy.linalg import eig

# Internal Parameters
RATIO = 50
ANGLE_TURN = 67.5*np.pi/180
STEP = 3

""" 
Begin for EDLine Functions
"""
# distance from point to line
def Distance_(a,b,point):
    if b is None:
        return np.abs(point[0] - a)
    else:
        return np.abs(b*point[0]  - point[1] + a)/np.sqrt(b*b + 1)
    
# fast fit line equation
def FitLine_(pixel_chain):
    x,y = zip(*pixel_chain)
    x = np.float64(x)
    y = np.float64(y)
    if np.dot(x-x.mean(),x-x.mean()) == 0: #if it is horizontal line
        beta = None
        alpha = x.mean()
        mse = ((x - alpha)**2).mean()
    else: # else ordinary line, MSE take the orthogonal distance
        beta = np.dot(x-x.mean(),y-y.mean())/np.dot(x-x.mean(),x-x.mean())
        alpha = np.mean(y) - beta*np.mean(x)
        mse = np.array([Distance_(alpha,beta,point)**2 for point in pixel_chain]).mean()
    return beta, alpha, mse

# EDLine detection
def EDLine(edges, minLineLen, lineFitErrThreshold = 1):
    # filter edges
    edges = [edge for edge in edges if len(edge)>=minLineLen]
    lines = []
    # finding initial line segment
    while edges:
        edge = edges.pop(0)
        while len(edge) >= minLineLen:
            b,a,err = FitLine_(edge[:minLineLen])
            if err < lineFitErrThreshold: # initial segment fournd
                break
            else: # otherwise move the window
                edge = edge[1:]
                
        if err < lineFitErrThreshold: # if line segment found, extend the line
            # start from segment
            line_len = minLineLen
            while line_len < len(edge):#and err < self.lineFitErrThreshold_:
                if Distance_(a,b,edge[line_len]) <= lineFitErrThreshold:
                    line_len+= 1
                    #b,a,err = self.FitLine(edge[:line_len])
                else:
                    break
            lines.append(edge[:line_len])
            # append the rest of pixels for next line extraction
            if len(edge[line_len:])>= minLineLen:
                edges.append(edge[line_len:])
    return lines

""" 
Begin for PLineD Functions
"""
# cos of angle between two vectors
def cosAngle_(v_1,v_2):
    return np.dot(v_1,v_2)/np.sqrt(np.dot(v_1,v_1)*np.dot(v_2,v_2))

# check if is a line using covariant matrix
def CovarianceCheck_(segment):
    cov_mat = np.cov(list(zip(*segment))) # covariance matrix
    eig_val, _= eig(cov_mat) # eigen value
    #if the ratio is large then return true, 1e-10 is a protection on dividing 0
    return (max(eig_val) + 1e-10)/(min(eig_val) + 1e-10) > RATIO

# Algorithm 1&2 segment cut and line detection
def SegmentFilter_(edges):
    lines = []
    for edge in edges:
        while edge:
            if len(edge) > 2*STEP:
                i = STEP
                while i < len(edge)-STEP:
                    v_1 = np.subtract(edge[i],edge[i-STEP])
                    v_2 = np.subtract(edge[i+STEP],edge[i])
                    if cosAngle_(v_1,v_2) < np.cos(ANGLE_TURN):
                        break # find segment
                    else: # step foward
                        i = i+STEP
                if i > len(edge)-STEP: # if the search is already to end
                    segment = edge # attach whole segment
                    break
                else: # cut the segement
                    segment = edge[:i] # cut the current segment
                    edge = edge[i:] # proceed to next search
            else:
                segment = edge # attach whole segment
                break
            # check if the segment is a line 
            if CovarianceCheck_(segment):
                lines.append(segment)
    return lines

# Algorithm 3. Group the lines
def GroupLines_(lines, min_L, min_P, tol_a, tol_d):
        # connect and merge the line inplace
        line_groups = []
        v_groups = []
        p1 = 0
        while p1 < len(lines):
            if len(lines[p1])>min_L:# if the segment begin to consider
                L_line = lines.pop(p1) # pop the current segment
                max_l = len(L_line) # longest line segment
                group_ = [L_line] # initiate group
                p_group = len(L_line) # total pixel in segment
                v_1 = np.subtract(L_line[-1], L_line[0]) # v_1 is the longest segment direction
                v_m = v_1 # mean direction
                p2 = 0
                while p2 < len(lines):
                    v_s = np.subtract(lines[p2][-1], lines[p2][0]) # v_s is the direstion of new segment s
                    # if they aligned in the same direction, compare with head-end
                    if np.abs(cosAngle_(v_m,v_s)) > np.cos(tol_a):
                        # distance of two vectors
                        v_1_n = (-v_m[1],v_m[0]) # normal of mean-line
                        mid_2 = 0.5*np.add(lines[p2][0], lines[p2][-1]) # mid point of line s
                        v_2 = np.subtract(mid_2,L_line[-1]) # direction to any point on the line
                        ds = np.abs(np.dot(v_1_n,v_2))/np.sqrt(np.dot(v_1_n,v_1_n)) # distance of segment to line
                        # if they are close enough, merge
                        if ds < tol_d:
                            N_line = lines.pop(p2) # pop the current segment
                            group_.append(N_line) # add to group
                            p_group += len(N_line) # update group member
                            if max_l > len(N_line): # update longest segment
                                L_line = N_line
                                v_1 = np.subtract(L_line[-1], L_line[0])
                                v_m += v_1
                        else:
                            p2 += 1 # manually poceed to next segment
                    else:
                        p2 += 1 # manually poceed to next segment
                if p_group > min_P: # if the group is large enough
                    line_groups.append(group_) # put in the line group
                    v_groups.append(v_m) # main direction of this group
                p1 = 0 # if group then reset p1
            else:
                p1 += 1 # next segment
        return line_groups, v_groups

# Algorithm 4: detect parallel groups
def ParallelGroups_(line_groups,v_groups, tol_a):
    parallel_groups = []
    while line_groups:
        l_1 = line_groups.pop(0) # pop the first element
        v_1 = v_groups.pop(0) 
        parallel_segments = [l_1] # initiate parallel segments
        p = 0 
        while p < len(line_groups):
            v_s = v_groups[p]
            if np.abs(cosAngle_(v_1,v_s)) > np.cos(tol_a):
                #add the segment to group and remove from pool
                parallel_segments.append(line_groups.pop(p)) 
                v_groups.pop(p)
            else:
                p += 1 # manually proceed
        if len(parallel_segments) > 1: # there are parallel groups
            for segments in parallel_segments:
                for s in  segments:
                    parallel_groups.append(s)
    # check the group
    return parallel_groups

# main body for PlineD
def PLineD(edges, min_L = 10, min_P = 1000, tol_a = 5*np.pi/180, tol_d = 60):
    # cut and filter line segments
    lines = SegmentFilter_(edges)
    #
    # group lines
    line_groups, v_groups = GroupLines_(lines, min_L, min_P, tol_a, tol_d)
    #return line_groups,v_groups
    # find parallel lines
    parallel_groups = ParallelGroups_(line_groups,v_groups, tol_a)
    # return
    return parallel_groups

import EdgeDrawing
import LineDetector as LD
import TestTool as TT
import cv2
import numpy as np
import glob
import time

# raw input 
#input_image = image

# ground truth folder
ground_truth_folder = 'test_data/IR_GT/'

# inflared Light
origin_image_folder = 'test_data/IR/'

# parameters for Edge Drawing
EDParam = {'ksize':3, # gaussian Smooth filter size if smoothed = False
           'sigma': 1, # gaussian smooth sigma ify smoothed = False
           'gradientThreshold': 25, # threshold on gradient image
           'anchorThreshold': 10, # threshold to determine the anchor
           'scanIntervals': 4} # scan interval, the smaller, the more detail
## Visible Light
#origin_image_folder = 'test_data/VL/'
#
## parameters for Edge Drawing (visible light)
#EDParam = {'ksize':3, # gaussian Smooth filter size if smoothed = False
#           'sigma': 1, # gaussian smooth sigma ify smoothed = False
#           'gradientThreshold': 25, # threshold on gradient image
#           'anchorThreshold': 10, # threshold to determine the anchor
#           'scanIntervals': 1} # scan interval, the smaller, the more detail

# image list
origin_image_list = sorted(glob.glob(origin_image_folder+'*.bmp'))
ground_truth_list = sorted(glob.glob(ground_truth_folder+'*.bmp'))
n = len(origin_image_list)

# initiate the EDLineDetector class
ED = EdgeDrawing.EdgeDrawing(EDParam)

""" 
Preprocess the images to get edges using EdgeDrawing
Also the test set is constructed for calculation of confusion matrix
"""
edges_list = []
edges_map_list = []
image_GT_list = []

start_time = time.time() # start timing
print('Preprocessing Image (EdgeDrawing)')

#Edge Drawing detection:
for i, image_file in enumerate(origin_image_list):
    # read image_file
    image = cv2.imread(image_file)
    # convert to gray-scale image
    input_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # edge Drawing
    edges,edges_map = ED.EdgeDrawing(input_image, smoothed = False)
    # edge list is a list of pixel chains it is used for EDLine and PLineD
    edges_list.append(edges)
    # edges map is a matrix with edges marked as 255, it is used for RHT
    edges_map_list.append(edges_map)
    #ground truth image
    image_ground_truth = cv2.imread(ground_truth_list[i])
    image_ground_truth = cv2.cvtColor(image_ground_truth,cv2.COLOR_BGR2GRAY) # convert to gray image
    image_GT_list.append(image_ground_truth)
    
    if (i+1)%100 == 0:
        print(str(i+1) + ' images processed')
        
elapsed_time = time.time() - start_time # end timming

# mean preprocess time for each image
mean_pre_time = elapsed_time/n

print('Preprocess Complete')
print('Time: {0:.1f}s. Average Time: {1:.3f}s'.format(elapsed_time, mean_pre_time))

#%%
""" 
Power Line detection Algorithm
"""

# parameters for RHT
distance_resolution = 1
angle_resolution = np.pi/180
minLineLength = 12
maxLineGap = 10
votes = 20

# Parameters for EDLine
minLineLen = 40
lineFitErrThreshold = 1.0

# parameters for PLineD
min_L = 5
min_P = 20
tol_a = 5*np.pi/180
tol_d = 3.0

print('---------Randomlized Hough------------')

TPR_list_HT = []
FPR_list_HT = []

start_time = time.time() # start timing
#
for i, edges_map in enumerate(edges_map_list):
    # RHT line detection
    lines = cv2.HoughLinesP(edges_map,distance_resolution,angle_resolution,votes,minLineLength,maxLineGap)
    lines = TT.TranslateLines(lines)
    # get prediction  label
    TP, FP, FN,_ = TT.ConfusionMatrix(lines,image_GT_list[i])
    if TP + FN > 4: # only count valid
        TPR_list_HT.append(TT.TPR(TP,FN))
    FPR_list_HT.append(TT.FPR(TP,FP))    
#
elapsed_time = time.time() - start_time # end time
# Note that the value of None will not count when calculate mean
print('Sensitivity: {0:.3f}'.format(np.array(TPR_list_HT).mean()))
print('Specificity: {0:.3f}'.format(1-np.array(FPR_list_HT).mean()))
print('Average time(including preprocessing): {0:.4f}s'.format(elapsed_time/n + mean_pre_time))

print('----------EDLine detection-------------')

TPR_list_ED = []
FPR_list_ED = []

start_time = time.time() # start timing

for i, edges in enumerate(edges_list):
    # EDline detection
    lines = LD.EDLine(edges, minLineLen,lineFitErrThreshold)
    # get prediction  label
    TP, FP, FN,_ = TT.ConfusionMatrix(lines,image_GT_list[i])
    if TP + FN > 4: #there is a coner dot wit 2x2 px in ground truth
        TPR_list_ED.append(TT.TPR(TP,FN))
    FPR_list_ED.append(TT.FPR(TP,FP))
    
elapsed_time = time.time() - start_time # end time

# when making the mean, it ignore the nan
print('Sensitivity: {0:.3f}'.format(np.array(TPR_list_ED).mean()))
print('Specificity: {0:.3f}'.format(1-np.array(FPR_list_ED).mean()))
print('Average time(including preprocessing): {0:.4f}s'.format(elapsed_time/n + mean_pre_time))

print('--------------PLineD---------------')

TPR_list_PD = []
FPR_list_PD = []

start_time = time.time() # start timing

for i, edges in enumerate(edges_list):
    # PLineD
    lines = LD.PLineD(edges,min_L, min_P,tol_a, tol_d)
    # get prediction  label
    TP, FP, FN, _ = TT.ConfusionMatrix(lines,image_GT_list[i])
    if TP > 4:
        TPR_list_PD.append(TT.TPR(TP,FN))
    FPR_list_PD.append(TT.FPR(TP,FP))

elapsed_time = time.time() - start_time # end time

print('Sensitivity: {0:.3f}'.format(np.array(TPR_list_PD).mean()))
print('Specificity: {0:.3f}'.format(1-np.array(FPR_list_PD).mean()))
print('Average time(including preprocessing): {0:.4f}s'.format(elapsed_time/n + mean_pre_time))

#retrieve uploaded file
uploaded = files.upload()
#print results
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

img = cv2.imread('16.jpg')
cv2_imshow(img)
cv2.waitKey(0)
img = cv2.blur(img,(2,2))
gray_seg = cv2.Canny(img, 0, 50)

import cv2
import numpy as np
from matplotlib import pyplot as plt

# read image
img = cv2.imread("16.jpg", 1)
# Find edge with Canny edge detection
edges = cv2.Canny(img, 100, 200)

# display results
plt.subplot(121), plt.imshow(img, cmap='gray')
plt.title('Original Image'), plt.xticks([]), plt.yticks([])
plt.subplot(122), plt.imshow(edges, cmap='gray')
plt.title('Edge Image'), plt.xticks([]), plt.yticks([])

plt.show()


import cv2
import numpy as np
#img = cv2.imread("test.png")
#img = cv2.imread("16.jpg")
#blurred = cv2.blur(img, (3,3))
#canny = cv2.Canny(blurred, 50, 200)

## find the non-zero min-max coords of canny
pts = np.argwhere(canny>0)
y1,x1 = pts.min(axis=0)
y2,x2 = pts.max(axis=0)

## crop the region
cropped = img[y1:y2, x1:x2]
cv2.imwrite("cropped.jpg", cropped)

tagged = cv2.rectangle(img.copy(), (x1,y1), (x2,y2), (0,255,0), 3, cv2.LINE_AA)
#cv2_imshow("tagged")
cv2_imshow("cropped.jpg")
#cv2.waitKey()

original_image = cv2.imread("16.jpg")
image = original_image.copy()
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
blurred = cv2.GaussianBlur(gray, (3, 3), 0)
canny = cv2.Canny(blurred, 120, 255, 1)

# Find contours in the image
cnts = cv2.findContours(canny.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
cnts = cnts[0] if len(cnts) == 2 else cnts[1]

# Obtain area for each contour
contour_sizes = [(cv2.contourArea(contour), contour) for contour in cnts]

# Find maximum contour and crop for ROI section
if len(contour_sizes) > 0:
    largest_contour = max(contour_sizes, key=lambda x: x[0])[1]
    x,y,w,h = cv2.boundingRect(largest_contour)
    cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)
    ROI = original_image[y:y+h, x:x+w]
    cv2_imshow(ROI) 

cv2_imshow(canny) 
cv2_imshow(image) 
cv2.waitKey(0)

import cv2
import pytesseract

image = cv2.imread('bangla.jpg')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
thresh = 255 - cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]

# Blur and perform text extraction(you can use raw image)
thresh = cv2.GaussianBlur(thresh, (3,3), 0)
data = pytesseract.image_to_string(thresh, lang='eng+ben', config='--psm 6')
print(data)

from PIL import Image    
import pytesseract as tess

print (tess.image_to_string(Image.open('bangla.jpg'), lang='Bengali'))

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

img = cv2.imread('16.jpg')
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
edges = cv2.Canny(gray,50,120)
minLineLength = 20
maxLineGap = 5
lines = cv2.HoughLinesP(edges,1,np.pi/180,100,minLineLength,maxLineGap)
#img_show = draw_vertical(edges, lines)
#for x1,y1,x2,y2 in lines[0]:
  #img_show=cv2.line(gray,(x1,y1),(x2,y2),(0,255,0),2)

i=cv2.findContours(gray)

cv2_imshow(edges)
cv2_imshow(img_show)
cv2.waitKey()
cv2.destroyAllWindows()

# Python code to detect an arrow (seven-sided shape) from an image. 
import numpy as np 
import cv2 
   
# Reading image 
img2 = cv2.imread('16.jpg', cv2.IMREAD_COLOR) 
   
# Reading same image in another variable and  
# converting to gray scale. 
img = cv2.imread('16.jpg', cv2.IMREAD_GRAYSCALE) 
   
# Converting image to a binary image  
# (black and white only image). 
_,threshold = cv2.threshold(img, 110, 255,  
                            cv2.THRESH_BINARY) 
   
# Detecting shapes in image by selecting region  
# with same colors or intensity. 
contours,_=cv2.findContours(threshold, cv2.RETR_TREE, 
                            cv2.CHAIN_APPROX_SIMPLE) 
   
# Searching through every region selected to  
# find the required polygon. 
for cnt in contours : 
    area = cv2.contourArea(cnt) 
   
    # Shortlisting the regions based on there area. 
    if area > 300:  
        approx = cv2.approxPolyDP(cnt,  
                                  0.009 * cv2.arcLength(cnt, True), True) 
   
        # Checking if the no. of sides of the selected region is 7. 
        if(len(approx) == 4):  
            cv2.drawContours(img2, [approx], 0, (0, 0, 255), 5) 
   
# Showing the image along with outlined arrow. 
cv2_imshow( img2)  
   
# Exiting the window if 'q' is pressed on the keyboard. 
if cv2.waitKey(0) & 0xFF == ord('q'):  
    cv2.destroyAllWindows() 

!pip install ocrd-fork-pylsd 

import cv2
import numpy as np
import os
from pylsd.lsd import lsd
from google.colab.patches import cv2_imshow
fullName = '3.jpg'
folder, imgName = os.path.split(fullName)
src = cv2.imread(fullName, cv2.IMREAD_COLOR)
gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
lines = lsd(gray)
array=[]
for i in range(lines.shape[0]):
    pt1 = (int(lines[i, 0]), int(lines[i, 1]))
    pt2 = (int(lines[i, 2]), int(lines[i, 3]))
    width = lines[i, 4]
    x=cv2.line(src, pt1, pt2, (0, 0, 255), int(np.ceil(width / 2)))
    array.append(x)
cv2.imwrite(os.path.join(folder, 'cv2_' + imgName.split('.')[0] + '.jpg'), src)
#cv2_imshow(x)

i=1
for x in array:
    print(i)
    
    cv2_imshow(x)
    i=i+1

from PIL import Image, ImageDraw
import numpy as np
import os
from pylsd.lsd import lsd
fullName = '3.jpg'
folder, imgName = os.path.split(fullName)
img = Image.open(fullName)
gray = np.asarray(img.convert('L'))
lines = lsd(gray)
draw = ImageDraw.Draw(img)
for i in range(lines.shape[0]):
    pt1 = (int(lines[i, 0]), int(lines[i, 1]))
    pt2 = (int(lines[i, 2]), int(lines[i, 3]))
    width = lines[i, 4]
    draw.line((pt1, pt2), fill=(0, 0, 255), width=int(np.ceil(width / 2)))
img.save(os.path.join(folder, 'PIL_' + imgName.split('.')[0] + '.jpg'))

cv2_imshow(draw)

 img = cv2.imread('book-2.jpg')
 gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
 blur = cv2.GaussianBlur(gray,(5,5),0)

thresh=0.5
edges = cv2.Canny(blur,thresh,thresh*2)
drawing = np.zeros(img.shape,np.uint8)  
contours,hierarchy = cv2.findContours(edges,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
for cnt in contours:
  x,y,w,h = cv2.boundingRect(cnt)
  cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)
  rect = cv2.minAreaRect(cnt)
  box = cv2.boxPoints(rect)
  box = np.int0(box)

